#===============================================================================
'''
Project:Lecture - Structural Wind Engineering WS19-20
        Chair of Structural Analysis @ TUM - R. Wuchner, M. Pentek

Author: mate.pentek@tum.de

Description: Convert Kratos level forces into ParOptBeam format

Created on:  06.01.2020
Last update: 26.05.2021
'''
#===============================================================================

import os
import numpy as np
from matplotlib.pylab import plt

# ===============================================================
# User input

# generated by the compute_compute_drag_process (Krato inbuilt)
# flow-attached results
file_name_inbuilt = 'FluidModelPart.Drag_structure_drag.dat'

# generated by the compute_global_force_process (custom)
# flow- and body- attached results
file_name_custom = 'FluidModelPart.Drag_structure_global_force.dat'

# generated by the compute_level_force_process (custom)
# flow- and body- attached results
n_level_files = 11
folder_name = 'level_force'
file_prefix = 'FluidModelPart.Drag_structure_level_force_'

# definition for the ParOptBeam output
# for how many elements (or nodes = elements +1) to create output
# input in form of a list to be able to generate various resolutions
#number_of_sampling_interval_cases = [1+1, 2+1, 3+1, 15+1, 30+1, 60+1]
number_of_sampling_interval_cases = [10+1]

# start and end node position
# for now only along (local - in ParOptBeam x - in Kratos z) the longitudial axis
start_point_position = [0.0, 0.0, 0.0]
end_point_position = [0.0, 0.0, 600.08]

#
output_file_prefix = 'force_dynamic_'

# ===============================================================
# Change with care -> change most probably not needed

# for the ParOptBeam formulation
# 3D
dofs_per_node = 6

output_type = 'body_attached'

# mapping Kratos result to ParOptBeam
axis_mapping = {
    # 0 -> fz
    0: 'fz',
    # 1 -> fx
    1: 'fx',
    # 2 -> fy
    2: 'fy',
    # 3 -> mz
    3: 'mz',
    # 4 -> mx
    4: 'mx',
    # 5 -> my
    5: 'my'}

# tolerance used for the comparison check
ABS_TOL = 1e-3
REL_TOL = 1e-12

# ===============================================================
# Check user input

n_files_with_prefix_in_folder = sum(file_prefix in fname for fname in os.listdir(folder_name))
if n_level_files == n_files_with_prefix_in_folder:
    print('Identical number of level files found as provided: ' + str(n_level_files))
else:
    raise Exception('Level files found ' + str(n_files_with_prefix_in_folder) + ' does not match with number of files to look for ' + str(n_level_files) + '!')

# can have less element (or nodes) and it will be interpolated
if n_level_files > max(number_of_sampling_interval_cases):
    raise Exception('ParOptBeam output can consider maximum the same amount of nodes as levels available!')

# ===============================================================
# Read-in existing

# for checking
# generated by the compute_compute_drag_process (Krato inbuilt)
# flow-attached results
ref_results_inbuilt = {
    't':  np.array(np.loadtxt(file_name_inbuilt, usecols=(0,))),
    'fx_f': np.array(np.loadtxt(file_name_inbuilt, usecols=(1,))),
    'fy_f': np.array(np.loadtxt(file_name_inbuilt, usecols=(2,))),
    'fz_f': np.array(np.loadtxt(file_name_inbuilt, usecols=(3,)))
}

# for checking
# generated by the compute_global_force_process (custom)
# flow- and body- attached results
ref_results_custom = {
    't':  np.array(np.loadtxt(file_name_custom, usecols=(0,))),
    'fx_f': np.array(np.loadtxt(file_name_custom, usecols=(1,))),
    'fy_f': np.array(np.loadtxt(file_name_custom, usecols=(2,))),
    'fz_f': np.array(np.loadtxt(file_name_custom, usecols=(3,))),
    'mx_f': np.array(np.loadtxt(file_name_custom, usecols=(4,))),
    'my_f': np.array(np.loadtxt(file_name_custom, usecols=(5,))),
    'mz_f': np.array(np.loadtxt(file_name_custom, usecols=(6,))),
    'fx_b': np.array(np.loadtxt(file_name_custom, usecols=(7,))),
    'fy_b': np.array(np.loadtxt(file_name_custom, usecols=(8,))),
    'fz_b': np.array(np.loadtxt(file_name_custom, usecols=(9,))),
    'mx_b': np.array(np.loadtxt(file_name_custom, usecols=(10,))),
    'my_b': np.array(np.loadtxt(file_name_custom, usecols=(11,))),
    'mz_b': np.array(np.loadtxt(file_name_custom, usecols=(12,)))
}

# for the actual conversion
# generated by the compute_level_force_process (custom)
# flow- and body- attached results

all_level_results = {}

for i in range(n_level_files):
    file_name = os.path.join(folder_name, file_prefix + str(i) + '.dat')

    # getting the center point coordinates for correct lever arm calculation
    with open(file_name) as f:
        for j, line in enumerate(f):
            if j == 1:
                third_line = f.readline()
                third_line = third_line.split()
                print(third_line)
                wait=input("check")
                break

    # NOTE: not explicitly needed to store intermediately
    all_level_results[i] = {
        'x': float(third_line[2][:-1]),
        'y': float(third_line[3][:-1]),
        'z': float(third_line[4]),
        't':  np.array(np.loadtxt(file_name, usecols=(0,))),
        'fx_f': np.array(np.loadtxt(file_name, usecols=(1,))),
        'fy_f': np.array(np.loadtxt(file_name, usecols=(2,))),
        'fz_f': np.array(np.loadtxt(file_name, usecols=(3,))),
        'mx_f': np.array(np.loadtxt(file_name, usecols=(4,))),
        'my_f': np.array(np.loadtxt(file_name, usecols=(5,))),
        'mz_f': np.array(np.loadtxt(file_name, usecols=(6,))),
        'fx_b': np.array(np.loadtxt(file_name, usecols=(7,))),
        'fy_b': np.array(np.loadtxt(file_name, usecols=(8,))),
        'fz_b': np.array(np.loadtxt(file_name, usecols=(9,))),
        'mx_b': np.array(np.loadtxt(file_name, usecols=(10,))),
        'my_b': np.array(np.loadtxt(file_name, usecols=(11,))),
        'mz_b': np.array(np.loadtxt(file_name, usecols=(12,)))
    }

# ===============================================================
# Own function definiton


def get_cumulated_results(multiple_level_results):
    # assumed to be ordered from lowest to highest level

    cumul_results = {}

    for i in range(len(multiple_level_results)):
        if i == 0:
            # initialize
            cumul_results['t'] = multiple_level_results[i]['t']

            for axis_def in ['_f', '_b']:
                cumul_results['fx' + axis_def] = multiple_level_results[i]['fx' + axis_def]
                cumul_results['fy' + axis_def] = multiple_level_results[i]['fy' + axis_def]
                cumul_results['fz' + axis_def] = multiple_level_results[i]['fz' + axis_def]
                cumul_results['mx' + axis_def] = multiple_level_results[i]['mx' + axis_def] - multiple_level_results[i]['fy' + axis_def] * multiple_level_results[i]['z']
                cumul_results['my' + axis_def] = multiple_level_results[i]['my' + axis_def] + multiple_level_results[i]['fx' + axis_def] * multiple_level_results[i]['z']
                cumul_results['mz' + axis_def] = multiple_level_results[i]['mz' + axis_def]

        else:
            # acumulate
            for axis_def in ['_f', '_b']:
                cumul_results['fx' + axis_def] = np.add(cumul_results['fx' + axis_def], multiple_level_results[i]['fx' + axis_def])
                cumul_results['fy' + axis_def] = np.add(cumul_results['fy' + axis_def], multiple_level_results[i]['fy' + axis_def])
                cumul_results['fz' + axis_def] = np.add(cumul_results['fz' + axis_def], multiple_level_results[i]['fz' + axis_def])
                cumul_results['mx' + axis_def] = np.add(cumul_results['mx' + axis_def], multiple_level_results[i]['mx' + axis_def] - multiple_level_results[i]['fy' + axis_def] * multiple_level_results[i]['z'])
                cumul_results['my' + axis_def] = np.add(cumul_results['my' + axis_def], multiple_level_results[i]['my' + axis_def] + multiple_level_results[i]['fx' + axis_def] * multiple_level_results[i]['z'])
                cumul_results['mz' + axis_def] = np.add(cumul_results['mz' + axis_def], multiple_level_results[i]['mz' + axis_def])

    return cumul_results


def enhance_ref_inbuilt(ref_res_inbuilt):
    for label in ref_res_inbuilt.keys():
        if label == 't':
            dt = ref_res_inbuilt[label][1]-ref_res_inbuilt[label][0]
            tval = ref_res_inbuilt[label][0] - dt
            ref_res_inbuilt[label] = np.insert(ref_res_inbuilt[label], 0, tval)
        else:
            ref_res_inbuilt[label] = np.insert(ref_res_inbuilt[label], 0, 0.0)

def compare_total_vs_cumulated(cumul_res, ref_res, plot_results=False):

    # using the labels of the refence files, as this will be different
    # depending whether the inbuilt or the custom process was used
    labels = [*ref_res]
    labels.remove('t')

    calc_error = {}
    abs_err = []
    rel_err = []
    for counter, label in enumerate(labels):
        abs_err.append(np.linalg.norm(np.subtract(ref_res[label],cumul_res[label])))
        rel_err.append(abs_err[-1] / np.max(np.abs(ref_res[label])))

        print('\tError [abs, rel] for ' + label + ': ' + ', '.join([str(val) for val in [abs_err[-1], rel_err[-1]]]))

    if np.mean(abs_err) < ABS_TOL and np.mean(rel_err) < REL_TOL:
        print('Errors ok')
    else:
        print('Mean abs error ' + str(np.mean(abs_err)))
        print('Mean rel error ' + str(np.mean(rel_err)))
        raise Exception('Errors too high, check input')

    if plot_results:
        for counter, label in enumerate(labels):
            plt.figure(counter)
            plt.title(label)
            plt.plot(ref_res['t'], ref_res[label], 'k.', alpha=0.5, label='ref')
            plt.plot(cumul_res['t'], cumul_res[label], 'r--', label='cumul')
            plt.legend()

        plt.show()


# ===============================================================
# Check read data


print('EVALUATED READ DATA')

cumul_res = get_cumulated_results(all_level_results)

enhance_ref_inbuilt(ref_results_inbuilt)
compare_total_vs_cumulated(cumul_res, ref_results_inbuilt)

compare_total_vs_cumulated(cumul_res, ref_results_custom)

wait = input("check...")


# ===============================================================
# PARAMETRIZATION


for number_of_sampling_intervals in number_of_sampling_interval_cases:
    print("RESULTS FOR CASE ", str(number_of_sampling_intervals))

    # setup the parametric space for the internal points on the line
    lower_bound = 0.0
    upper_bound = 1.0

    # initialize equidistant intervals
    my_param = [1.0] * (number_of_sampling_intervals)
    # overwrite first and last as endpoints are to be included
    my_param[0] = 0.5
    my_param[-1] = 0.5

    parametrized_internal_points = [lower_bound + x*(upper_bound-lower_bound)/(
        number_of_sampling_intervals-1) for x in range(number_of_sampling_intervals + 1)]

    parametrized_internal_points = [0.0] * (len(my_param)+1)

    for i in range(len(my_param)):
        for j in range(i+1):
            parametrized_internal_points[i+1] += my_param[j]

    parametrized_internal_points = [
        x / parametrized_internal_points[-1] for x in parametrized_internal_points]

    # print("Interval parameter: ", my_param)
    # print("Running parameter: ", parametrized_internal_points)

    # determining the positions of the output points
    direction_vector = [
        x - y for x, y in zip(end_point_position, start_point_position)]

    current_positions = []
    for k in range(len(parametrized_internal_points)):
        current_positions.append(
            [x + parametrized_internal_points[k]*y for x, y in zip(start_point_position, direction_vector)])

    levels = {}
    running_x_coords = []
    for idx in range(len(current_positions)-1):
        levels[idx] = {}
        levels[idx]['start'] = current_positions[idx]
        levels[idx]['end'] = current_positions[idx+1]

        # first interval
        if idx == 0:
            levels[idx]['center'] = [
                x1 for x1 in levels[idx]['start']]
        elif idx == (len(current_positions)-1)-1:
            levels[idx]['center'] = [
                x2 for x2 in levels[idx]['end']]
        else:
            levels[idx]['center'] = [
                (x1+x2)/2 for x1, x2 in zip(levels[idx]['start'], levels[idx]['end'])]

        levels[idx]['output_file'] = None

        # the center is where the forces are computed
        # this has to correspond to the x-coords of the beam
        running_x_coords.append(levels[idx]['center'][2])


    # ===============================================================
    # CHECK PARAMETRIZATION


    print('X coords: ', len(running_x_coords))
    print(running_x_coords)

    ints = [y-x for x, y in zip(running_x_coords[:-1], running_x_coords[1:])]
    print('Intervals: ', len(ints))
    print(ints)


    # ===============================================================
    # INITIALIZATION OF NEW DATA

    # level force
    case = number_of_sampling_intervals
    folder_name = str(case)
    file_prefix = 'level_'
    all_new_level_results = {}

    # NOTE: for the actual run
    for i in range(case):
        all_new_level_results[i] = {
            'x': 0.0,
            'y': 0.0,
            'z': running_x_coords[i],
            't': all_level_results[0]['t'],
            'fx_f': np.zeros(len(all_level_results[0]['t'])),
            'fy_f': np.zeros(len(all_level_results[0]['t'])),
            'fz_f': np.zeros(len(all_level_results[0]['t'])),
            'mx_f': np.zeros(len(all_level_results[0]['t'])),
            'my_f': np.zeros(len(all_level_results[0]['t'])),
            'mz_f': np.zeros(len(all_level_results[0]['t'])),
            'fx_b': np.zeros(len(all_level_results[0]['t'])),
            'fy_b': np.zeros(len(all_level_results[0]['t'])),
            'fz_b': np.zeros(len(all_level_results[0]['t'])),
            'mx_b': np.zeros(len(all_level_results[0]['t'])),
            'my_b': np.zeros(len(all_level_results[0]['t'])),
            'mz_b': np.zeros(len(all_level_results[0]['t']))
        }


    # ===============================================================
    # CALCULATION OF NEW LOADS


    # NOTE: assuming that
    # all_new_level_results -> has to have fewer points (levels)
    # all_level_results -> has to have more points (levels)
    for i in range(len(all_new_level_results)-1):

        for j in range(len(all_level_results)):
            if (j < len(all_level_results)-1 and all_new_level_results[i]['z']<= all_level_results[j]['z'] and all_level_results[j]['z'] < all_new_level_results[i+1]['z']) or (j == len(all_level_results)-1 and all_new_level_results[i]['z']<= all_level_results[j]['z'] and all_level_results[j]['z'] <= all_new_level_results[i+1]['z']):
                # print('Level ' + str(j) + ' is in between levels ' + str(i) + ' and ' + str(i+1) +'\n')

                # creating weighting factors for linear weighing of the results
                # dist to lower neighbour -> will be positive
                a = all_level_results[j]['z'] - all_new_level_results[i]['z']
                # dist to upper neighbour -> will be negative
                b = all_level_results[j]['z'] - all_new_level_results[i+1]['z']
                # above need to take abs() of value
                l = abs(a) + abs(b)
                fctr_lower = abs(b/l)
                fctr_upper = abs(a/l)

                for axis_def in ['_f', '_b']:
                    # cumulate results
                    # lower neighbour
                    all_new_level_results[i]['fx' + axis_def] = np.add(all_new_level_results[i]['fx' + axis_def], fctr_lower * all_level_results[j]['fx' + axis_def])
                    all_new_level_results[i]['fy' + axis_def] = np.add(all_new_level_results[i]['fy' + axis_def], fctr_lower * all_level_results[j]['fy' + axis_def])
                    all_new_level_results[i]['fz' + axis_def] = np.add(all_new_level_results[i]['fz' + axis_def], fctr_lower * all_level_results[j]['fz' + axis_def])
                    all_new_level_results[i]['mx' + axis_def] = np.add(all_new_level_results[i]['mx' + axis_def], fctr_lower * (all_level_results[j]['mx' + axis_def] - all_level_results[j]['fy' + axis_def] * a))
                    all_new_level_results[i]['my' + axis_def] = np.add(all_new_level_results[i]['my' + axis_def], fctr_lower * (all_level_results[j]['my' + axis_def] + all_level_results[j]['fx' + axis_def] * a))
                    all_new_level_results[i]['mz' + axis_def] = np.add(all_new_level_results[i]['mz' + axis_def], fctr_lower * all_level_results[j]['mz' + axis_def])

                    # upper neighbour
                    all_new_level_results[i+1]['fx'+ axis_def] = np.add(all_new_level_results[i+1]['fx' + axis_def], fctr_upper * all_level_results[j]['fx' + axis_def])
                    all_new_level_results[i+1]['fy'+ axis_def] = np.add(all_new_level_results[i+1]['fy' + axis_def], fctr_upper * all_level_results[j]['fy' + axis_def])
                    all_new_level_results[i+1]['fz'+ axis_def] = np.add(all_new_level_results[i+1]['fz' + axis_def], fctr_upper * all_level_results[j]['fz' + axis_def])
                    all_new_level_results[i+1]['mx'+ axis_def] = np.add(all_new_level_results[i+1]['mx' + axis_def], fctr_upper * (all_level_results[j]['mx' + axis_def] - all_level_results[j]['fy' + axis_def] * b))
                    all_new_level_results[i+1]['my'+ axis_def] = np.add(all_new_level_results[i+1]['my' + axis_def], fctr_upper * (all_level_results[j]['my' + axis_def] + all_level_results[j]['fx' + axis_def] * b))
                    all_new_level_results[i+1]['mz'+ axis_def] = np.add(all_new_level_results[i+1]['mz' + axis_def], fctr_upper * all_level_results[j]['mz' + axis_def])

            else:
                print('Level ' + str(j) + ' IS NOT in between levels ' + str(i) + ' and ' + str(i+1) +'\n')
                pass

    print('EVALUATED NEW RESULTS FOR CASE: ', str(number_of_sampling_intervals))
    compare_total_vs_cumulated(get_cumulated_results(all_new_level_results), ref_results_custom)
    wait = input("check...")


    # ===============================================================
    # SAVE LOADS IN *.NPY format

    if output_type == 'flow_attached':
        axis_def = '_f'
    elif output_type == 'body_attached':
        axis_def = '_b'
    else:
        raise Exception('Output type not possible!')

    dynamic_force = np.zeros([number_of_sampling_intervals * dofs_per_node, len(all_new_level_results[0]['t'])])
    counter = 0
    for i in range(len(all_new_level_results)):
        for pob_idx, kr_idx in axis_mapping.items():
            dynamic_force[i * dofs_per_node + pob_idx, :] = all_new_level_results[i][kr_idx + axis_def]

    np.save(output_file_prefix + str(number_of_sampling_intervals), dynamic_force)

np.save('array_time', all_new_level_results[0]['t'])